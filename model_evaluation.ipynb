{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theav\\miniconda3\\envs\\modelfinetune\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\theav\\miniconda3\\envs\\modelfinetune\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\theav\\miniconda3\\envs\\modelfinetune\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import Trainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import the metrics\n",
    "from rouge_score import rouge_scorer\n",
    "from rouge_score.scoring import Score\n",
    "from bert_score import BERTScorer\n",
    "from readability import Readability\n",
    "#from evaluation.readability import flesch_kincaid_grade_level, dale_chall_readability_score, coleman_liau_index, lens\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Metrics\n",
    "- **Relevance**: ROUGE (1, 2, and L) and BERTScore\n",
    "- **Readability**: Flesch-Kincaid Grade Level (FKGL) and Dale-Chall Readability Score (DCRS), Coleman-Liau Index (CLI), and LENS\n",
    "- **Factuality**: AlignScore, SummaC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Samsung/samsum\", \"samsum\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theav\\miniconda3\\envs\\modelfinetune\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load our first fine-tuned model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"samsum_bart_base\"\n",
    "model = BartForConditionalGeneration.from_pretrained(f\"models/{model_name}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the summaries from disk\n",
    "with open(\"models/samsum_bart_base/summaries.json\", \"r\") as f:\n",
    "    summaries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_summaries(model, tokenizer, dataset, limit: int = None, batch_size: int = 8) -> np.ndarray:\n",
    "    limit = len(dataset) if limit is None else min(limit, len(dataset))\n",
    "    summaries = np.empty(limit, dtype=object)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    num_batches = (limit + batch_size - 1) // batch_size\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Generating summaries\"):\n",
    "            # Get batch indices\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, limit)\n",
    "            dialogues = [dataset[i][\"dialogue\"] for i in range(start_idx, end_idx)]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            inputs = tokenizer(dialogues, return_tensors=\"pt\", max_length=512, truncation=True, padding=True).to(model.device)\n",
    "            \n",
    "            # Generate summaries for the batch\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                predicted_summaries = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=150, \n",
    "                    num_beams=4, \n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            # Decode summaries and store them\n",
    "            decoded_summaries = [tokenizer.decode(summary, skip_special_tokens=True) for summary in predicted_summaries]\n",
    "            summaries[start_idx:end_idx] = decoded_summaries\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"{article}\"\n",
    "generated_summaries = generate_summaries(model, tokenizer, dataset['test'], limit=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue: Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "True Summary: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "Generated Summary: Hannah is looking for Betty's number. Amanda can't find it. Larry called her last time they were at the park together.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dialogue: {dataset['test'][0]['dialogue']}\")\n",
    "print(f\"True Summary: {dataset['test'][0]['summary']}\")\n",
    "print(f\"Generated Summary: {generated_summaries[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 words => Summary Trypanosomatids are parasites that cause African sleeping sickness, Nagana cattle disease, South-American Chagas’ disease and leishmaniasis. These parasites are caused by a protein called trypanothione, which is found in the mitochondrion of the Trypanosome. This protein is made up of a group of enzymes called thioredoxin reductases, which are found in many different types of cells. The enzymes that are involved in this process are known as tryparedoxin and Tpx. However, it was not clear how the two enzymes work together. To investigate this question, Manta et al. used a technique called “tryparedoxin redox reductase� is too short.\n"
     ]
    }
   ],
   "source": [
    "for summary in generated_summaries:\n",
    "    #get the number of words in the summary\n",
    "    summary_length = len(summary.split())\n",
    "    \n",
    "    if summary_length < 100:\n",
    "        print(f\"{summary_length} words => Summary {summary} is too short.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = summaries['generated_summaries'][0]\n",
    "ts = summaries['summary'][0]\n",
    "t = summaries['dialogue'][0]\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "score1 = rouge.score(t, gs)\n",
    "score2 = rouge.score(ts, gs)\n",
    "score3 = rouge.score(t, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(precision=0.8260869565217391, recall=0.24358974358974358, fmeasure=0.37623762376237624)\n",
      "Score(precision=0.34782608695652173, recall=0.5, fmeasure=0.41025641025641024)\n",
      "Score(precision=0.5625, recall=0.11538461538461539, fmeasure=0.19148936170212766)\n"
     ]
    }
   ],
   "source": [
    "for s in [score1, score2, score3]:\n",
    "    print(s['rouge1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute all metrics for the test set and save them to a json file\n",
    "#the dataset is a list of dicts with 'article', 'summary', 'section_headings', 'keywords', 'year', 'title'\n",
    "#add a progress bar\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "def compute_metrics(summaries: list[str], dataset):\n",
    "    metrics = {\"relevance\": {\"rouge\": [], \"bert\": []}, \"readability\": {\"fkgl\": [], \"dcrs\": [], \"cli\": []}}\n",
    "    \n",
    "    bert = BERTScorer(lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    for i in tqdm(range(len(summaries)), desc=\"Computing metrics\"):\n",
    "        #compute the predicted summary from the article\n",
    "        article = dataset[i][\"article\"]\n",
    "        expert_summary = dataset[i][\"summary\"]\n",
    "        predicted_summary = summaries[i]\n",
    "        \n",
    "        if(len(predicted_summary.split()) < 100):\n",
    "            continue\n",
    "        \n",
    "        #compute the relevance scores\n",
    "        metrics[\"relevance\"][\"rouge\"].append(rouge.score(expert_summary, predicted_summary))\n",
    "        metrics[\"relevance\"][\"bert\"].append(bert.score([predicted_summary], [expert_summary])[0].item())\n",
    "        \n",
    "        #compute the readability scores\n",
    "        r = Readability(predicted_summary)\n",
    "        metrics[\"readability\"][\"fkgl\"].append(r.flesch_kincaid().score)\n",
    "        metrics[\"readability\"][\"dcrs\"].append(r.dale_chall().score)\n",
    "        metrics[\"readability\"][\"cli\"].append(r.coleman_liau().score)\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\theav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_summaries_below_word_thershold(generated_summaries: np.ndarray, threshold: int = 100) -> np.ndarray:\n",
    "    filtered_summaries = []\n",
    "    \n",
    "    filtered_summaries = generated_summaries[generated_summaries.split().__len__() > threshold]\n",
    "    \n",
    "    return filtered_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theav\\miniconda3\\envs\\modelfinetune\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Computing metrics: 100%|██████████| 241/241 [00:17<00:00, 13.48it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics(generated_summaries, dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.5631067961165048, recall=0.31868131868131866, fmeasure=0.4070175438596491), 'rouge2': Score(precision=0.14634146341463414, recall=0.08264462809917356, fmeasure=0.10563380281690142), 'rougeL': Score(precision=0.2912621359223301, recall=0.16483516483516483, fmeasure=0.21052631578947367)}\n"
     ]
    }
   ],
   "source": [
    "print(metrics['relevance']['rouge'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the metrics json\n",
    "model_folder = f\"models/{model_name}\"\n",
    "\n",
    "with open(f\"{model_folder}/metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelfinetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
